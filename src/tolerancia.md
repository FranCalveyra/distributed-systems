# Tolerancia a fallos
Se espera que el sistema tenga fallas: es inevitable. Lo que se busca es que sea tolerable a las mismas.

## Confiabilidad y sus requerimientos
### Dependable Systems

High availability + Reliability =  Dependability 

Tolerancia a fallas:
* Tolerar fallas parciales
* Tener la capacidad de auto recuperarse

(Ver definiciones de clases anteriores de distintos conceptos relacionados a disponibilidad)

El concepto de "Dependability", yendo al ejemplo de un auto, se pone en juego en caso de que, si bien el motor puede andar bien y tener buena potencia, la bater칤a quiz치 falle todo el tiempo.

### M칠tricas

* Mean Time To Failure (MTTF): Tiempo medio hasta el fallo
	* EJ: en un datacenter, se mide cu치l es el tiempo promedio hasta la falla de un disco, para poder reemplazarlo antes.
* Mean Time To Repair (MTTR): Tiempo medio para reparar.
* Mean Time Between Failures (MTBF): MTTF + MTTR
* Disponibilidad (A): MTTF / (MTTF + MTTR)

![Time To Failure](./assets/tolerancia/time_to_failure.png)

#### M칠tricas - ejercicio

Availability en 1 mes = 99,95% uptime

쮺u치ntos minutos en el mes puede estar ca칤do el sistema? Es decir, que no responde o responde con 500s (Internal Server Error).

쮺u치ntos minutos hay en 30 d칤as? 43.200 minutos

Respuesta:
* Tiempo disponible = 43.200 * 99.95% = 43.178,4 minutos
* Tiempo indisponible = 43.200 - 43.178,4 = 43.200 * (1 - 99.95%)
* Tiempo indisponible = 21,6 minutos

쮼s mucho o poco tiempo? **Depende**. Si es algo cr칤tico, s칤  (p. ej. el funcionamiento de respiradores en un hospital). En caso de que no, quiz치 no sea tan grave.

Es muy com칰n que empresas serias muestren su uptime en una p치gina web (p. ej. Mercado Libre).

## Teorema de CAP

Un sistema distribuido solo puede garantizar simult치neamente dos de las siguientes tres propiedades:

1. C - **Consistencia** (Consistency): Todos los nodos del sistema ven la misma versi칩n de los datos al mismo tiempo. Es como si hubiera una 칰nica copia actualizada de los datos
2. A - **Disponibilidad** (Availability): Todas las solicitudes reciben una respuesta (no un error), aunque esa respuesta no siempre contenga la versi칩n m치s reciente de los datos
3. P - **Tolerancia a Particiones** (Partition Tolerance): El sistema contin칰a funcionando incluso si algunos nodos no pueden comunicarse entre s칤 (una "partici칩n" de la red).

### Consistencia, disponibilidad y particionamiento

* Sistemas **CP**: Priorizan la Consistencia. Si hay una partici칩n, el sistema puede volverse no disponible para garantiszar que las respuestas sean correctas.
	* EJ: sistema bancario, donde es preferible tardar en mostrarle al usuario su balance correcto, antes de mostrarle uno desactualizado y que haga una transacci칩n incorrecta.
* Sistemas **AP**: Priorizan la Disponibilidad. Si hay una partici칩n, el sistema puede devolver datos potencialmente inconsistentes para seguir respondiendo.
	* EJ: un Marketplace como Mercado Libre, donde yo quiz치 hago una compra de algo que en realidad no tiene stock. Sin embargo, despu칠s puede arreglarse con una compensaci칩n (EJ: un vale de compra por $\$5000$)


## Fallas

쯈u칠 es una falla?

* Un sistema **falla (failure)** cuando no cumple sus promesas o especificaciones.
	* Es lo m치s abstracto, porque puede no ser causada por un error del sistema en s칤. La falla es un comportamiento que yo no esperaba del sistema.
	* De la falla no te salv치s (?)
* Un **error** es una parte del estado del sistema que puede llevar a un fallo (p. ej. un bug de programaci칩n)
* Un **defecto (fault)** es la causa de un error (p. ej. un programador que introduce un bug, un disco rallado, etc.)

La cadena de sucesos es: Defecto -> Error -> Falla.

Es imporatnte distinguirlos porque es la forma en la cual uno puede hacerse tolerante a cada uno de estos 3 problemas.
### Categorizaci칩n de fallas

![Categor칤as de Fallas 1](./assets/tolerancia/failure_categories_1.png)
![Categor칤as de Fallas 2](./assets/tolerancia/failure_categories_2.png)

1. **Fallas por ca칤da (Crash Failures)**: se detiene y no responde
2. **Fallos de Omisi칩n (Omission failures**)
	1. Fallos de env칤o (Send-omission): falla al enviar un mensaje que deb칤a enviar
	2. Fallos de recepci칩n (Receive-omission): falla al recibir mensajes que llegan
		1. Cuando tengo una sobrecarga a nivel de OS (porque tengo demasiados threads laburando), se me puede producir un fallo de este tipo, ya que no tengo capacidad suficiente para recibir.
3. **Fallos de tiempo (Timing failures)**: timeout. Se tarda mucho en responder un mensaje.
	1. La diferencia entre omisi칩n y timing es que en el caso del primero falla instant치neamente.
4. **Fallos de respuesta**: respuesta es incorrecta
	1. Fallos de valor (Value Failure): La respuesta contiene un valor incorrecto. Yo quer칤a un determinado valor, y me responden cualquier cosa
	2. Fallos de transici칩n de estado: se desv칤a el correcto flujo de control
5. **Fallos arbitrarios**: puede producir respuestas incorrectas en momentos arbitrarios (ej: respuestas inconsistentes). Este tipo de error es el peor, porque como los errores no siguen un patr칩n, son dif칤ciles de detectar. Siendo rigurosos, no existen los fallos arbitrarios, porque siempre hay una causa. Lo que realmente pasa es que yo como desarrollador desconozco la causa.
	1. Falla por omisi칩n: Un componente no realiza una acci칩n que deber칤a hacer
	2. Falla por comisi칩n: Un componente realiza una acci칩n que no deber칤a haber realizado.

> Fun fact 游뱁: Los fallos arbitrarios tambi칠n se llaman _bizantinos_

### Detecci칩n de fallas

De suma importancia para la tolerancia a fallos.
* **Dificultad en sistemas as칤ncronos**: en un sistema puramente as칤ncrono, es imposible distinguir si un proceso se ha ca칤do o si solo est치 respondiendo muy lento.
	* Algo que se suele hacer es medir el "heartbeat": el load balancer env칤a requests cada cierto tiempo para verificar si el servicio est치 vivo. Cuando no respondi칩 ante X intentos, se considera muerto. 
* Mecanismos comunes: la pr치ctica se basa en mecanismos de tiempo de espera (timeouts) para sospechar que un proceso ha fallado.
#### Forma de detecci칩n

En los casos siguientes, la severidad es ascendente.

Imaginemos que un proceso `P` intenta detectar si un proceso `Q` ha fallado:

1. **Fallas fail-stop**: pueden detectarse de forma confiable, asumiendo correcta comunicaci칩n y un retraso m치ximo en las respuestas de `Q`.
2. **Fallas fail-noisy**: `P` solo eventualmente llega a la conclusi칩n correcta de que `Q` ha fallado. Puede haber un tiempo desconocido durante el cual las detecciones de `P` sobre el comportamiento de `Q` no son fiables
3. **Fallas fail-silent**: Se asume que los enlaces de comunicaci칩n no tienen fallas, pero el proceso `P` no puede distinguir entre fallas por ca칤da y fallas por omisi칩n (no sabe si est치 ca칤do o si por alg칰n motivo no est치 respondiendo)
4. **Fallas fail-safe**: fallas arbitrarias de `Q`, pero son benignas: no pueden causar ning칰n da침o.
5. **Fallas fail-arbitrary**: las fallas de `Q` pueden ser inobservables adem치s de ser perjudiciales para el comportamiento correcto de otros procesos.

## Masking (manejo de fallos) mediante redundancia

1. **Redundancia de informaci칩n**: Se a침ade informaci칩n redundante (p. ej. c칩digos de detecci칩n o correcci칩n de errores, como en TCP)
2. **Redundancia de tiempo**: Se realiza una acci칩n varias veces (p. ej. retransmisi칩n de mensajes).
3. **Redundancia f칤sica**: Se usan componentes duplicados (p. ej. replicaci칩n de procesos o datos).

## Redundancia

Redundancia no es lo mismo que replicaci칩n:
* **Redundante**: se usa al mismo tiempo que algo
* **Replicado**: es una copia de algo

쯈u칠 pasa cuando le pido lo mismo a X sistemas distintos? Espero que respondan lo mismo. **Ese es el principal problema de la redundancia**, porque quiero ser consistente con mis respuestas.

![TMR Circuit](./assets/tolerancia/triple_modular_redundancy_circuit.png)

- Siguiendo el caso de la imagen, incluso el votador se replica, porque es capaz de caerse tambi칠n.

Evidentemente la redundancia tiene un costo

### Y en Software?

* **Server redundancy**: M칰ltiples instancias
* **Data Redundancy**: M칰ltiples copias de la DB
* **Network Redundancy**: M칰ltiples redes por si una se cae
* Otros...

Ejemplo: un servidor por pa칤s
- Puedo tener m치s disponibilidad porque distribuye la carga
- Pero no tengo redundancia. Se cae el servidor de un pa칤s y ese pa칤s queda sin servicio

## Recuperaci칩n ante fallos

Tiene m치s relaci칩n con una p칠rdida de datos que con una ca칤da de la aplicaci칩n. Consiste en c칩mo se vuelve a un estado razonable despu칠s de haber tenido una p칠rdida de datos.

### Recuperaci칩n del sistema

Objetivo:
* Analizar las dependencias entre servicios para entender c칩mo reaccionar ante un evento de recuperaci칩n de uno de los miembros.
* **Dependencia de datos entre aplicaciones**

El problema principal de la recuperaci칩n ante fallas es la **consistencia entre los nodos del sistema**. Por ejemplo, si un servicio de ventas depende de ciertos datos consistentes que tiene un servicio de historial, al caerse este 칰ltimo, el primer servicio no puede realizar operaciones consistentes.

### Recuperaci칩n

* La recuperaci칩n se refiere al proceso de devolver un componente fallido (o todo el sistema) a un estado correcto luego de que un fallo haya ocurrido, y haya sido detectado/reparado
* Lo que se tiene como **objetivo** es minimizar el impacto y restaurar el servicio
* Tenemos distintos tipos de recuperaci칩n:
  * Hacia atr치s (backwards): volver a un estado anterior correcto
  * Hacia adelante (forward): es necesario conocer los errores que van a ocurrir con antelaci칩n
    * Es un quilombo de implementar porque tampoco pod칠s pretender "atajar todos los penales" <img src="https://media.tycsports.com/files/2022/12/18/517235/emiliano-martinez_862x485_wmk.webp" height="40" width="=60"/>
#### Backwards Recovery - Checkpoint
* El sistema debe guardar su estado peri칩dicamente: **checkpoint**
* En caso de fallo, un proceso puede retroceder a su 칰ltimo checkpoint v치lido en lugar de reiniciar desde cero
* La recuperaci칩n requiere construir un **estado global consistente** a partir de los estados locales guardados por cada proceso
* Tenemos 2 maneras principales de tomar checkpoints:
  * **De manera coordinada**: usando un algoritmo como 2PC. Es m치s simple que tenga un estado global consistente
  * **De manera independiente**: el desaf칤o justamente yace en lograr un estado global consistente
* La l칤nea de recuperaci칩n (**recovery line**) es la colecci칩n m치s reciente de puntos de control que forman un estado global consistente.
* Es recomendable volver el estado a una l칤nea de recuperaci칩n.
* Es m치s simple el checkpoint coordinado y por lo tanto m치s usado.

##### Desaf칤os del checkpoint independiente
- Cada proceso graba su estado local ocasionalmente y de manera no coordinada.
- Para descubrir una l칤nea de recuperaci칩n, cada proceso debe retroceder a su estado guardado m치s reciente.
- Si estos estados locales conjuntamente no forman un estado global consistente requiere que otro proceso retroceda a un estado anterior.
- Esto puede desencadenar un efecto **domin칩**.

### Message Logging
El checkpointing tradicional tiene un par de limitiaciones:
- **Costo Elevado**: La toma de checkpoints es una operaci칩n costosa y puede penalizar severamente el rendimiento.
- **No Determinismo**: Si solo se usa checkpointing, el comportamiento despu칠s de la recuperaci칩n puede ser diferente al original debido a que pueden recibirse los mensajes en orden o tiempos diferentes de los que se hab칤a recibido antes de la falla

Para "parchear" esas fallas que tiene el checkpoint, surge el `Message Logging`.
#### 쯈u칠 es?
* **Idea clave**: Si la transmisi칩n de mensajes puede ser "reproducida" (replayed), se puede restaurar un estado consistente global.
* **Mecanismo**: 
	* Se parte de un estado previamente checkpointed
	* Todos los mensajes enviados desde ese checkpoint se retransmiten y manejan o ejecutan de nuevo
* Beneficios: 
	* Permite restaurar un estado m치s all치 del checkpoint m치s reciente sin el alto costo de nuevos checkpoints
	* Facilita una reproducci칩n exacta de los eventos
	* Mayor eficiencia en la pr치ctica, al requerir menos checkpoints.

## Resiliencia de Procesos

- **Foco**: 쮺칩mo hacer que un proceso o un conjunto de procesos sea tolerante a fallos?
- **Objetivo**: hacer que un grupo de procesos se comporte como un 칰nico proceso m치s robusto.
- **Metodolog칤a**: se logra mediante la replicaci칩n de procesos
- **Desaf칤o**: mantener la coherencia y coordinaci칩n entre los procesos replicados para que act칰en como una sola entidad fiable.

### Organizaci칩n de grupos

Dos opciones:
1. Todos los procesos son iguales, decisiones colectivas (ej: muchos sistemas P2P)
2. Un coordinador (l칤der) y trabajadores (p. ej. DNS, primary-backup).

Gesti칩n de membres칤a:
* Crear/eliminar grupos, unir/dejar procesos. Centralizado (servidor de grupo) vs Descentralizado

### Algoritmos de consenso

En definitiva, tengo varios procesos redundantes, y necesito una sola respuesta.
* **Supuesto**: en un sistema tolerante a fallos, todos los procesos ejecutan los mismos comandos, en el mismo orden, de igual manera que todo el resto de los procesos sin errores.
* **Problema**: 쮺칩mo conseguir consenso sobre el comando concreto a ejecutar?
* **Enfoques**: el algoritmo de consenso elegido depende del modelo de fallo que el sistema debe tolerar
#### K-Fault Tolerant
Un sistema es **_k-fault tolerant_** si puede sobrevivir a fallos en k componentes y seguir cumpliendo sus especificaciones
- fail-silent: $k+1$ componentes
- fail-safe: un m칤nimo de $2k+1$ procesos

Si un sistema k-fault tolerant sufre la ca칤da de m치s de $k$ miembros, no se puede confiar en sus resultados

> En unga-unga, si yo te digo "flaquito, fijate que tengo 10 componentes en mi arquitectura y sigo andando si se me caen 5". En la que se me cayeron 6, es imposible confiar en los resultados de mi sistema.



### Fallos por ca칤da

| **Protocolo/Enfoque**           | **Fallos tolerados**                                                                                      | **Requisitos m칤nimos**                                          |
| ------------------------------- | --------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| Basado en inundaci칩n (Flooding) | Fail-stop(detectables de forma fiable)                                                                    | -                                                               |
| Raft                            | Fail-noisy (la ca칤da es detectada correctamente en alg칰n momento)                                         | -                                                               |
| Paxos                           | Fallos por ca칤da (crash failures). Tambi칠n asume comunicaci칩n no fiable y sistemas parcialmente s칤ncronos | $2k + 1$ servidores para tolerar $k$ fallos de ca칤da silenciosa |

### Fallos arbitrarios
Estos son los fallos m치s graves, donde un servidor puede actuar de manera inconsistente o maliciosa, produciendo resultados incorrectos que no pueden detectarse inmediatamente.


| **Protocolo/Enfoque**                          | **Fallos tolerados**                          | **Requisitos m칤nimos**                               |
| ---------------------------------------------- | --------------------------------------------- | ---------------------------------------------------- |
| Tolerancia Pr치ctica a Fallos Bizantinos (PBFT) | Fallos Arbitrarios (Bizantinos).              | $3k + 1$ procesos para tolerar k fallos arbitrarios. |
| HotStuff                                       | Fallos Arbitrarios (como una mejora de PBFT). | $3k + 1$ procesos                                    |

### Consenso en sistemas de Blockchain

| Tipo de blockchain | Enfoque de consenso                                                                     | Contexto                                                                                  |
| ------------------ | --------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| Sin permiso        | Basado en elecci칩n de l칤der (p. ej. Prueba de Trabajo PoW, Prueba de Participaci칩n PoS) | El l칤der decide qu칠 bloque se a침ade                                                       |
| Con permiso        | PBFT o variantes m치s escalables (p. ej. HotStuff)                                       | Usado por grupos peque침os de procesos tolerantes a fallos que deciden qui칠n a침ade bloques |
### Limitaciones

- **Rendimiento**: replicaci칩n y organizaci칩n de procesos implican una p칠rdida potencial de rendimiento debido a la gran cantidad de mensajes que deben intercambiarse para alcanzar el consenso.
- **Procesos as칤ncrono**: es imposible alcanzar el consenso en un sistema as칤ncrono si existe incluso un 칰nico proceso defectuoso (no podemos diferenciar si est치 ca칤do o est치 lento)
- **Teorema CAP**: en los sistemas distribuidos se espera que hayan particiones, lo que obliga a elegir entre `C` o `A`

## Comunicaci칩n cliente-servidor confiable

Antes hablamos de errores en los procesos. 쯈u칠 pasa si fallan los canales de comunicaci칩n?

- Nos interesa poder manejar los errores.
- Asegurar que los mensajes se procesen una sola vez o al menos de manera predecible.

Examinemos las 2 categor칤as: Point To Point y RPC
1. Point to Point (conexi칩n dedicada)
	1. Confiabilidad basada en el protocolo de comunicaci칩n TCP
	2. Ante un fallo se puede intentar establecer conexi칩n
2. RPC
   1. **Objetivo**: hacer parecer que estamos haciendo llamadas locales

### Posibles errores - RPC

1. No se puede conectar con el servidor
2. Se pierde el mensaje enviado por el cliente
3. El servidor muere luego de recibir el mensaje
4. El mensaje de respuesta del servidor se pierde
5. El cliente muere luego de enviar el mensaje

### Posibles soluciones - RPC

1. Handlear una excepci칩n. Pero se tendr칤a la ilusi칩n de estar usando un servidor local
2. Configurar un timeout en el cliente con alg칰n retry.
3. Timeout con entry, 쯣ero c칩mo asegur치s que no haya un reproceso?
	1. At least once
	2. At most once
	3. No guarantees
4. Timeout con retry
	1. Retransmisi칩n
	2. Operaciones idempotentes. No siempre se puede (ver saldo vs transferir plata)
	3. N칰mero de secuencia en los mensajes. Requiere mantener estado
	4. Bit de retransmisi칩n agregado a la solicitud
5. Existen algunas propuestas complejas y con limitaciones. Se recomienda no hacer nada y que, luego del reinicio, se pueda volver a un estado anterior a su reinicio (ver [Recuperaci칩n](#recuperaci칩n)).

>Respecto a la idempotencia, refiere que al ejecutarse dos veces una solicitud, tenga el mismo efecto.
> - **La idempotencia** es una propiedad del mensaje, aplicada cuando es realmente importante (p. ej. en streaming, ya de base no me interesa volver a enviar un frame si fall칩)
> - El ejemplo m치s claro es una transferencia bancaria, donde tengo que asegurar que una transacci칩n no se ejecute dos veces.
> - Como todo, la idempotencia tiene sus costos.

## Comunicaci칩n de grupo o multicast confiable

## Definici칩n

* Servicio que permite enviar mensajes a m칰ltiples receptores en un sistema distribuido
* Esencial para la tolerancia a fallos 

Tenemos como **objetivo principal** asegurar que los mensajes se entreguen a todos los miembros de un grupo particular


### 쮺칩mo realizamos la comunicaci칩n?

![Comunicaci칩n Multicast](./assets/tolerancia/multicast_connections.png)
Se realiza una conexi칩n point to point para cada proceso. Puede parecer una soluci칩n, pero no realmente. Se queda corta en _escalabilidad_.

- Si enviamos m칰ltiples solicitudes en paralelo, puede escalar

### Orden en los mensajes

* Un mensaje enviado a nu grupo debe ser entregado a cada miembro no defectuoso de ese grupo, o a ninguno de ellos si el remitente falla durante la transmisi칩n. Eso se llama: **sincron칤a virtual**, establece "todo o nada".
* Pero no asegura el orden de entrega entre mensajes. Para ello se distinguen cuatro tipos principales de ordenamiento
	1. Multicasts no ordenados (Unordered multicasts)
	2. Multicasts ordenados FIFO
	3. Multicasts ordenados causalmente (Casually ordered)
	4. Entrega totalmente ordenada (Totally ordered): todos los mensajes son entregados en orden a todos los miembros del grupo, independientemente del origen o de su causalidad.

La sincron칤a virtual que ofrece una entrega totalmente ordenada se llama **Atomic Multicast**.

6 formas diferentes de comunicaci칩n de grupo (multicast) confiable o sincron칤a virtual

| Multicast     | ordenamiento | Entrega totalmente ordenada |
| ------------- | ------------ | --------------------------- |
| Confiable     | No           | No                          |
| FIFO          | FIFO         | No                          |
| Causal        | Causal       | No                          |
| Atomic        | No           | S칤                          |
| FIFO Atomic   | FIFO         | S칤                          |
| Causal Atomic | Causal       | S칤                          |


## Transacciones en servicios

Contexto
* Sistemas complejos con relaciones entre ellos
	* Muchas veces tiene que operar sincronizados
* Ejemplos:
	* Reservar hotel y vuelo
* 쮺칩mo aseguro que se haga todo o nada?
	* Cada uno es una operaci칩n distinta

Dos operaciones en principio:
* Commit distribuido y at칩mico
	* Complicado hacerlo en ciertos casos distintos a una base de datos
	* EJ: algoritmo two-phase commit (2PC), pero en la pr치ctica es un problema m치s que una soluci칩n
    	* Involucra 1 coordinar y $N$ participantes
* Compensaciones

### Compensaciones

Deshacer una operaci칩n ya realizada. Esto es una gesti칩n para deshacer algo ya realizado. Consiste en "ir desarmando" lo que se fue haciendo. Es una consistencia de negocio donde se deshace lo que se hizo.

* No es un rollback, es decir, no hay un *undo* de la operaci칩n en s칤.
* EJ de situaci칩n - Despegar.com: reservo el hotel, pero el pasaje ya lo saqu칠 y est치 commiteado
* Esta opci칩n es la m치s com칰n en sistemas distribuidos.

Ejemplos de compensaci칩n:
* Env칤o un mail para avisar
* Hago un contramovimiento
